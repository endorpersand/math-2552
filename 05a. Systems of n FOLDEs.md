**Textbook sections**: 6.1, 6.2
This lesson extends [[03a. Systems of 2 FOLDEs|Systems of 2 FOLDEs]].

# Terminology

## Matrix functions
Matrices who elements are functions:
$$
P(t) = \begin{bmatrix}p_{11}(t) & \ldots & p_{1n}(t) \\ \vdots & & \vdots \\ p_{n1}(t) & \ldots & p_{nn}(t)\end{bmatrix}
$$
*Differentiation and integration are elementwise*.

## Expressing $n$th Order DE as Linear System
Extends [[03a. Systems of 2 FOLDEs#2nd Order Linear DEs as Linear Systems|2nd Order Linear DEs as Linear Systems]].

$$y^{(4)} + y = \sin(t)$$

Set the variables:
$$
\begin{align*}
x_1 &= y\\
x_2 &= y'\\
x_3 &= y''\\
x_4 &= y^{(3)}
\end{align*}
$$
Then, construct the differential identities:
$$
\begin{align*}
x_1' &= x_2\\
x_2' &= x_3\\
x_3' &= x_4
\end{align*}
$$
And the DE:
$$
x_4' = -x_1 + \sin(t)
$$

Altogether:
$$
\frac{d\vec x}{dt} = \begin{bmatrix}0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ -1 & 0 & 0 & 0\end{bmatrix}\vec x + \begin{bmatrix}0 \\ 0 \\ 0 \\ \sin(t)\end{bmatrix}
$$

# Uniqueness of Solution to Linear System

This section extends [[02c. Existence & Uniqueness of 1st Order IVPs#Existence & Uniqueness of 1st Order Linear IVPs|Existence & Uniqueness of 1st Order Linear IVPs]].
If $P$ and $\vec{g}$ are continuous on $(\alpha, \beta)$, $t_0 \in (\alpha, \beta)$, there is a unique solution to the IVP
$$
\vec x' = P(t)\vec x + \vec g (t),\quad \vec x(t_0) = \vec x_0
$$

*Proof goes beyond the scope of this course.*

> [!Example]+
> Identify an interval on which a unique solution will exist for
> $$
(t - 2)y'' + 3y = t,\quad y(0) = 0, y'(0) = 1
> $$
> 
> $$\frac{d\vec{x}}{dt} = \begin{bmatrix}0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ -\frac{3}{t-2} & 0 & 0 & 0\end{bmatrix} - \begin{bmatrix}0 \\ 0 \\ 0 \\ \frac{t}{{t-2}}\end{bmatrix}$$
> So, there must be a unique solution for $(-\infty, 2)$.

# Linear Independence of Scalar and Vector Functions

## of Scalar Functions

This  section extends [[03b. The Wronskian#The Wronskian & Linear Independence|The Wronskian]] and [[03b. The Wronskian#Fundamental Set of Solutions|Fundamental Set of Solutions]].
Let $y_1, y_2, \ldots, y_n$ be the solutions to $y^{(n)} + p_1 y^{(n - 1)} + \ldots + p_ny = 0$.
If these solutions are linearly independent in given interval $I$, they form the **fundamental set of solutions** for this equation in interval $I$, and every solution is some linear combination of these solutions.

To determine if they are linearly dependent. Note that for 3 solutions:
$$c_1 y_1 + c_2 y_2 + c_3 y_3 = 0$$
However, this is 1 equation, 3 unknowns. We also know that...
$$
\begin{align*}
c_1 y_1' + c_2 y_2' + c_3 y_3' &= 0\\
c_1 y_1'' + c_2 y_2'' + c_3 y_3'' &= 0
\end{align*}
$$
This can be expressed as:
$$
\begin{bmatrix}
y_1 & y_2 & y_3 \\
y_1' & y_2' & y_3' \\
y_1'' & y_2'' & y_3''
\end{bmatrix} \begin{bmatrix}c_1 \\ c_2 \\ c_3 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0\end{bmatrix}
$$

**The Wronskian**, for $n$ solutions is:
$$
W[y_1, y_2, \ldots, y_n] = \begin{vmatrix}y_1 & y_2 & \ldots & y_n \\ y_1' & y_2' & \ldots & y_n' \\ \vdots & \vdots & \ddots & \vdots \\ y_1^{(n - 1)} & y_2^{(n - 1)} & \ldots & y_n^{(n - 1)} \end {vmatrix}
$$
If this computes to 0 for a given interval $I$, then there must be a linear combination and therefore linear dependence in that interval.

> [!Example]+
> Determine whether the functions are linearly independent.
> 1. $y_1 = e^t, y_2 = e^{-2t}, y_3 = 3e^t - 2e^{-2t}$
> 
> $y_3 = 3y_1 - 2y_2$. So these are linearly dependent.
> 
> 2. $y_1 = t, y_2 = t^2, y_3 = 1-2t^2$
> 
> $$\begin{bmatrix}t & t^2 & 1 - 2t^2 \\ 1 & 2t & - 4t \\ 0 & 2 & -4\end{bmatrix}\vec c = \vec 0$$
> If the determinant is $0$, then the functions must be linearly dependent.
> $$\det A = t(-8t + 8t) - 1(-4t^2 - 2 + 4t^2) = 2$$

## of Vector Functions

Let $\vec x_1, \vec x_2, \ldots, \vec x_n$ be solutions to
$$
\vec x = P(t)\vec x + \vec g(t)
$$
on interval $I$. These are **the fundamental set of solutions** if the Wronskian ($W[\vec x_1, \vec x_2, \ldots, \vec x_n]$) is non-zero.

> [!Example]+
> $$
> \begin{matrix}\vec y_1 = \begin{bmatrix}t \\ 1 - t\\ 0\end{bmatrix} & \vec y_2 = \begin{bmatrix}0 \\ 1 \\ t\end{bmatrix} & \vec y_3 = \begin{bmatrix}t \\ 3 - t \\ 2t\end{bmatrix}\end{matrix}
> $$
> If these vectors are linearly independent then:
> $$\begin{bmatrix}\vec y_1 & \vec y_2 & \vec y_3\end{bmatrix} \vec c = \vec 0$$ only if $\vec c = \vec 0$.
> 
> ($\det \begin{bmatrix}\vec y_1 & \vec y_2 & \vec y_3\end{bmatrix} = 0$, you can figure out $\vec{c}$ by inspection.)

